{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f85dc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "994ee3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_all = np.loadtxt('Audiobooks_data.csv', delimiter = ',')\n",
    "unscaled_input_all = raw_data_all[:, 1: -1]\n",
    "target_all = raw_data_all[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6d2d02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_one_data = int(np.sum(target_all))\n",
    "zero_target_counter = 0\n",
    "indicase_to_remove = []\n",
    "\n",
    "for i in range (target_all.shape[0]):\n",
    "    if target_all[i] == 0:\n",
    "        zero_target_counter += 1\n",
    "        if zero_target_counter >= num_one_data:\n",
    "            indicase_to_remove.append(i)\n",
    "            \n",
    "unscaled_input_all_balance = np.delete(unscaled_input_all, indicase_to_remove, axis = 0)\n",
    "target_all_balance = np.delete(target_all, indicase_to_remove, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd320b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_input = preprocessing.scale(unscaled_input_all_balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a00f9a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indicase = np.arange(unscaled_input_all_balance.shape[0])\n",
    "np.random.shuffle(shuffled_indicase)\n",
    "\n",
    "shuffled_input = unscaled_input_all_balance[shuffled_indicase]\n",
    "shuffled_target = target_all_balance[shuffled_indicase]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92a8927f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1801.0 3578 0.5033538289547235\n",
      "215.0 447 0.4809843400447426\n",
      "221.0 448 0.4933035714285713\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "samples_count = shuffled_input.shape[0]\n",
    "train_sample_count = int(0.8 * samples_count)\n",
    "validation_sample_count = int(0.1 * samples_count)\n",
    "test_sample_count = samples_count - train_sample_count - validation_sample_count\n",
    "\n",
    "#train_size = 0.8\n",
    "input_train, input_rem, target_train, target_rem = train_test_split(shuffled_input, shuffled_target, train_size = 0.8)\n",
    "\n",
    "#test_size = 0.5\n",
    "input_validation, input_test, target_validation, target_test = train_test_split(input_rem, target_rem, test_size = 0.5)\n",
    "\n",
    "print(np.sum(target_train), train_sample_count, np.sum(target_train/train_sample_count))\n",
    "print(np.sum(target_validation), validation_sample_count, np.sum(target_validation/validation_sample_count))\n",
    "print(np.sum(target_test), test_sample_count, np.sum(target_test/test_sample_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5afcbd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('Audiobooks_data_train', inputs = input_train, targets = target_train)\n",
    "np.savez('Audiobooks_data_validation', inputs = input_validation, targets = target_validation)\n",
    "np.savez('Audiobooks_data_test', inputs = input_test, targets = target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e791c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "npz = np.load('Audiobooks_data_train.npz')\n",
    "train_input, train_target = npz['inputs'].astype(np.float_), npz['targets'].astype(np.int_)\n",
    "\n",
    "npz = np.load('Audiobooks_data_validation.npz')\n",
    "validation_input, validation_target = npz['inputs'].astype(np.float_), npz['targets'].astype(np.int_)\n",
    "\n",
    "npz = np.load('Audiobooks_data_test.npz')\n",
    "test_input, test_target = npz['inputs'].astype(np.float_), npz['targets'].astype(np.int_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20790ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class that will do the batching for the algorithm\n",
    "# This code is extremely reusable. You should just change Audiobooks_data everywhere in the code\n",
    "class Audiobooks_Data_Reader():\n",
    "    # Dataset is a mandatory arugment, while the batch_size is optional\n",
    "    # If you don't input batch_size, it will automatically take the value: None\n",
    "    def __init__(self, dataset, batch_size = None):\n",
    "    \n",
    "        # The dataset that loads is one of \"train\", \"validation\", \"test\".\n",
    "        # e.g. if I call this class with x('train',5), it will load 'Audiobooks_data_train.npz' with a batch size of 5.\n",
    "        npz = np.load('Audiobooks_data_{0}.npz'.format(dataset))\n",
    "        \n",
    "        # Two variables that take the values of the inputs and the targets. Inputs are floats, targets are integers\n",
    "        self.inputs, self.targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n",
    "        \n",
    "        # Counts the batch number, given the size you feed it later\n",
    "        # If the batch size is None, we are either validating or testing, so we want to take the data in a single batch\n",
    "        if batch_size is None:\n",
    "            self.batch_size = self.inputs.shape[0]\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "        self.curr_batch = 0\n",
    "        self.batch_count = self.inputs.shape[0] // self.batch_size\n",
    "    \n",
    "    # A method which loads the next batch\n",
    "    def __next__(self):\n",
    "        if self.curr_batch >= self.batch_count:\n",
    "            self.curr_batch = 0\n",
    "            raise StopIteration()\n",
    "            \n",
    "        # You slice the dataset in batches and then the \"next\" function loads them one after the other\n",
    "        batch_slice = slice(self.curr_batch * self.batch_size, (self.curr_batch + 1) * self.batch_size)\n",
    "        inputs_batch = self.inputs[batch_slice]\n",
    "        targets_batch = self.targets[batch_slice]\n",
    "        self.curr_batch += 1\n",
    "        \n",
    "        # One-hot encode the targets. In this example it's a bit superfluous since we have a 0/1 column \n",
    "        # as a target already but we're giving you the code regardless, as it will be useful for any \n",
    "        # classification task with more than one target column\n",
    "        classes_num = 2\n",
    "        targets_one_hot = np.zeros((targets_batch.shape[0], classes_num))\n",
    "        targets_one_hot[range(targets_batch.shape[0]), targets_batch] = 1\n",
    "        \n",
    "        # The function will return the inputs batch and the one-hot encoded targets\n",
    "        return inputs_batch, targets_one_hot\n",
    "    \n",
    "        \n",
    "    # A method needed for iterating over the batches, as we will put them in a loop\n",
    "    # This tells Python that the class we're defining is iterable, i.e. that we can use it like:\n",
    "    # for input, output in data: \n",
    "        # do things\n",
    "    # An iterator in Python is a class with a method __next__ that defines exactly how to iterate through its objects\n",
    "    def __iter__(self):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "167c1c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1, Training Loss: 55.273, Validation Loss: 4.202, Validation Accuracy: 69.351%\n",
      "Epoch2, Training Loss: 2.873, Validation Loss: 1.528, Validation Accuracy: 65.772%\n",
      "Epoch3, Training Loss: 0.937, Validation Loss: 0.427, Validation Accuracy: 79.642%\n",
      "Epoch4, Training Loss: 0.492, Validation Loss: 0.408, Validation Accuracy: 78.747%\n",
      "Epoch5, Training Loss: 0.505, Validation Loss: 0.475, Validation Accuracy: 75.392%\n",
      "End of Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1751: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_23232\\1633223120.py:13: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.inputs, self.targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_23232\\1633223120.py:13: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.inputs, self.targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "input_size = 10\n",
    "output_size = 2\n",
    "hidden_layer_size = 50\n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "inputs = tf.compat.v1.placeholder(tf.float32, shape = (None, input_size))\n",
    "targets = tf.compat.v1.placeholder(tf.int32, shape = (None, output_size))\n",
    "\n",
    "weights_1 = tf.compat.v1.get_variable(\"weights_1\", [input_size, hidden_layer_size])\n",
    "biases_1 = tf.compat.v1.get_variable(\"biases_1\", [hidden_layer_size])\n",
    "outputs_1 = tf.nn.relu(tf.matmul(inputs, weights_1) + biases_1)\n",
    "\n",
    "weights_2 = tf.compat.v1.get_variable(\"weights_2\", [hidden_layer_size, hidden_layer_size])\n",
    "biases_2 = tf.compat.v1.get_variable(\"biases_2\", [hidden_layer_size])\n",
    "outputs_2 = tf.nn.relu(tf.matmul(outputs_1, weights_2) + biases_2)\n",
    "\n",
    "weights_3 = tf.compat.v1.get_variable(\"weights_3\", [hidden_layer_size, output_size])\n",
    "biases_3 = tf.compat.v1.get_variable(\"biases_3\", [output_size])\n",
    "outputs = tf.matmul(outputs_2, weights_3) + biases_3\n",
    "\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits = outputs, labels = targets)\n",
    "mean_loss = tf.reduce_mean(loss)\n",
    "\n",
    "optimize = tf.compat.v1.train.AdamOptimizer(learning_rate = 0.001).minimize(mean_loss)\n",
    "out_equals_target = tf.equal(tf.argmax(outputs, 1), tf.argmax(targets, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(out_equals_target, tf.float32))\n",
    "\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "sess = tf.compat.v1.InteractiveSession()\n",
    "initializer = tf.compat.v1.global_variables_initializer()\n",
    "sess.run(initializer)\n",
    "\n",
    "batch_size = 100\n",
    "max_epochs = 100\n",
    "prev_validation_loss = 9999999.\n",
    "\n",
    "train_data = Audiobooks_Data_Reader('train', batch_size)\n",
    "validation_data = Audiobooks_Data_Reader('validation')\n",
    "\n",
    "for epoch_counter in range(max_epochs):\n",
    "    \n",
    "    curr_epoch_loss = 0.\n",
    "    \n",
    "    for input_batch, target_batch in train_data:\n",
    "        _, batch_loss = sess.run([optimize, mean_loss],\n",
    "            feed_dict = {inputs: input_batch, targets: target_batch})\n",
    "        \n",
    "        curr_epoch_loss += batch_loss\n",
    "        \n",
    "    curr_epoch_loss /= train_data.batch_count\n",
    "    \n",
    "    validation_loss = 0.\n",
    "    validation_accuracy = 0.\n",
    "    \n",
    "    for input_batch, target_batch in validation_data:\n",
    "        validation_loss, validation_accuracy = sess.run([mean_loss, accuracy],\n",
    "        feed_dict = {inputs: input_batch, targets: target_batch})\n",
    "        \n",
    "    print('Epoch' + str(epoch_counter + 1) + \n",
    "         ', Training Loss: ' + '{0:.3f}'.format(curr_epoch_loss) +\n",
    "         ', Validation Loss: ' + '{0:.3f}'.format(validation_loss) +\n",
    "         ', Validation Accuracy: ' + '{0:.3f}'.format(validation_accuracy * 100) + '%')\n",
    "    \n",
    "    if validation_loss > prev_validation_loss:\n",
    "        break\n",
    "    \n",
    "    prev_validation_loss = validation_loss\n",
    "    \n",
    "print('End of Training') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3af6fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 74.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_23232\\1633223120.py:13: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.inputs, self.targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_23232\\1633223120.py:13: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.inputs, self.targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "test_data = Audiobooks_Data_Reader('test')\n",
    "\n",
    "for input_batch, target_batch in test_data:\n",
    "    test_accuracy = sess.run([accuracy],\n",
    "    feed_dict = {inputs: input_batch, targets: target_batch})\n",
    "\n",
    "test_accuracy_percent = test_accuracy[0] * 100.\n",
    "print('Test Accuracy: ' + '{:.2f}'.format(test_accuracy_percent) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2109b1ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
